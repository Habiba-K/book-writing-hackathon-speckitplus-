# Implementation Tasks: Embedding Pipeline Setup

**Feature**: 001-embedding-pipeline
**Date**: 2025-12-17
**Plan**: [plan.md](./plan.md)
**Spec**: [spec.md](./spec.md)
**Generated by**: `/sp.tasks`

## Dependencies

User Story completion order: US1 → US2 → US3

## Parallel Execution Examples

- **US1**: `get_all_urls` and `extract_text_from_url` can be developed in parallel
- **US2**: `chunk_text` and `embed` functions can be developed in parallel
- **US3**: `create_collection` and `save_chunk_to_qdrant` can be developed in parallel

## Implementation Strategy

MVP scope includes User Story 1 (content extraction) as a complete, testable pipeline component. Subsequent user stories add embedding and storage capabilities. Each user story is independently testable with clear acceptance criteria.

---

## Phase 1: Setup

- [X] T001 Create backend directory structure in repository root
- [X] T002 Initialize Python project with UV in backend directory
- [X] T003 Add required dependencies to pyproject.toml: cohere, qdrant-client, requests, beautifulsoup4, python-dotenv
- [X] T004 Create .env file template with COHERE_API_KEY, QDRANT_URL, QDRANT_API_KEY placeholders
- [X] T005 Create empty main.py file in backend directory with constants defined
- [X] T006 [P] Install UV package manager if not already installed

## Phase 2: Foundational Components

- [X] T007 Create environment variable loading function in main.py
- [X] T008 [P] Initialize Cohere client with error handling
- [X] T009 [P] Initialize Qdrant client with error handling
- [X] T010 Create deterministic ID generation function for upsert capability
- [X] T011 [P] Add logging configuration for pipeline progress tracking
- [X] T012 Create retry mechanism with exponential backoff for API calls

## Phase 3: [US1] Extract Content from Docusaurus Site

**Goal**: Extract text content from a deployed Docusaurus documentation site, discovering and visiting all documentation pages while excluding navigation elements.

**Independent Test Criteria**: Can be fully tested by pointing the crawler at a live Docusaurus URL and verifying that extracted text matches the visible page content (excluding navigation, headers, footers, and HTML artifacts).

**Acceptance Scenarios**:
1. Given a Docusaurus site URL, when the crawler processes the site, then it discovers and visits all documentation pages following internal links.
2. Given a documentation page with markdown content, when the extractor processes the page, then it extracts the main content body excluding navigation menus, sidebars, footers, and boilerplate.
3. Given extracted content with HTML artifacts, when text cleaning is applied, then the output contains only clean, readable text with preserved paragraph structure and code blocks.
4. Given a page that has already been crawled, when re-running the extraction, then the system skips duplicate pages based on URL normalization.

- [X] T013 [US1] Implement get_all_urls function to parse sitemap.xml
- [X] T014 [P] [US1] Create test sitemap parsing with sample XML
- [X] T015 [US1] Implement extract_text_from_url function with BeautifulSoup
- [X] T016 [P] [US1] Add CSS selectors for Docusaurus content elements (article, main)
- [X] T017 [P] [US1] Add exclusion selectors for navigation, footer, sidebar elements
- [X] T018 [US1] Implement text cleaning to remove HTML artifacts and normalize whitespace
- [X] T019 [P] [US1] Add error handling for failed page fetches with retry logic
- [X] T020 [US1] Implement URL normalization to handle duplicates
- [X] T021 [US1] Add logging to track crawl progress and extracted content length
- [X] T022 [US1] Test content extraction with sample Docusaurus pages

## Phase 4: [US2] Generate Embeddings with Cohere

**Goal**: Generate semantic embeddings from extracted documentation text using Cohere's embedding models to enable semantic search capabilities.

**Independent Test Criteria**: Can be fully tested by providing sample text chunks and verifying that Cohere returns valid embedding vectors of the expected dimensions.

**Acceptance Scenarios**:
1. Given cleaned text content from a documentation page, when the text is sent to the embedding service, then a vector embedding is returned with the correct dimensions.
2. Given a long document exceeding the embedding model's token limit, when processing occurs, then the document is automatically chunked into appropriate segments with overlap for context preservation.
3. Given multiple text chunks to embed, when batch processing is invoked, then embeddings are generated efficiently in batches to minimize API calls.
4. Given an API rate limit or temporary failure, when embedding generation is attempted, then the system retries with exponential backoff.

- [X] T023 [US2] Implement chunk_text function for text segmentation
- [X] T024 [P] [US2] Add configurable chunk_size and overlap parameters
- [X] T025 [US2] Implement embed function to call Cohere API
- [X] T026 [P] [US2] Configure Cohere model (embed-english-v3.0) with proper input types
- [X] T027 [P] [US2] Add batch processing for efficient API calls
- [X] T028 [US2] Implement rate limiting and retry logic for Cohere API
- [X] T029 [US2] Add validation to ensure returned embeddings have correct dimensions (1024)
- [X] T030 [US2] Test embedding generation with sample text chunks

## Phase 5: [US3] Store Embeddings in Qdrant

**Goal**: Store generated embeddings in Qdrant vector database to enable efficient semantic similarity searches for RAG retrieval.

**Independent Test Criteria**: Can be fully tested by inserting sample embeddings with metadata and verifying successful retrieval via similarity search queries.

**Acceptance Scenarios**:
1. Given an embedding vector with associated metadata (source URL, page title, text chunk), when the embedding is stored, then Qdrant persists the vector and metadata in the specified collection.
2. Given a Qdrant collection does not exist, when the pipeline runs, then the collection is automatically created with appropriate vector configuration.
3. Given stored embeddings in Qdrant, when a similarity search query is performed, then the most semantically similar documents are returned with their metadata.
4. Given an embedding that already exists (same source URL and chunk), when re-indexing occurs, then the existing entry is updated rather than duplicated.

- [X] T031 [US3] Implement create_collection function for Qdrant
- [X] T032 [P] [US3] Configure collection with 1024-dimension vectors and cosine distance
- [X] T033 [US3] Implement save_chunk_to_qdrant function for upsert operations
- [X] T034 [P] [US3] Add proper payload structure with text, url, title, chunk_index
- [X] T035 [P] [US3] Implement upsert logic using deterministic chunk IDs
- [X] T036 [US3] Add error handling for Qdrant connection issues
- [X] T037 [US3] Test Qdrant storage with sample embeddings and metadata
- [X] T038 [US3] Verify upsert behavior works correctly for re-indexing

## Phase 6: [US1] [US2] [US3] Integration and Main Pipeline

**Goal**: Orchestrate the complete pipeline from sitemap parsing through content extraction, embedding generation, and vector storage.

- [X] T039 Implement main function to orchestrate the complete pipeline
- [X] T040 [P] Add configuration constants for sitemap URL, collection name, chunk size, etc.
- [X] T041 [P] Add progress logging for each stage of the pipeline
- [X] T042 Connect get_all_urls to extract_text_from_url processing
- [X] T043 Connect extract_text_from_url to chunk_text processing
- [X] T044 Connect chunk_text to embed processing
- [X] T045 Connect embed to save_chunk_to_qdrant processing
- [X] T046 Add summary statistics at pipeline completion
- [X] T047 Test complete pipeline with target site: https://book-writing-hackathon-speckitplus.vercel.app/

## Phase 7: Polish & Cross-Cutting Concerns

- [X] T048 Add comprehensive error handling throughout the pipeline
- [X] T049 [P] Add input validation for all function parameters
- [X] T050 [P] Add performance timing for each pipeline stage
- [X] T051 Add command-line argument support for configuration
- [X] T052 Update README with setup and usage instructions
- [X] T053 Add .gitignore for backend directory (excluding .env)
- [X] T054 Perform final integration test with all 29 pages
- [X] T055 Verify pipeline completes within 5-minute performance goal